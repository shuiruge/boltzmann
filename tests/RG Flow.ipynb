{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from boltzmann.data.mnist import load_mnist\n",
    "from boltzmann.generic.base import train\n",
    "from boltzmann.generic.bernoulli import (\n",
    "    BernoulliBoltzmannMachine, HintonInitializer, initialize_fantasy_state,\n",
    "    get_reconstruction_error, LatentIncrementingInitializer, enlarge_latent)\n",
    "from boltzmann.utils import History, ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "IMAGE_SIZE = (16, 16)\n",
    "LATENT_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "SEED = 42\n",
    "\n",
    "INCREMENT = 8\n",
    "\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "(X, y), _ = load_mnist(image_size=IMAGE_SIZE, binarize=True, minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(epochs: int, cache_path: str = None):\n",
    "    ambient_size = IMAGE_SIZE[0] * IMAGE_SIZE[1]\n",
    "    bm = BernoulliBoltzmannMachine(\n",
    "        ambient_size=ambient_size,\n",
    "        latent_size=LATENT_SIZE,\n",
    "        initializer=HintonInitializer(X),\n",
    "        max_step=100,\n",
    "        tolerance=1e-1,\n",
    "        connect_ambient_to_ambient=False,\n",
    "        sync_ratio=0.25,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    if cache_path is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "        epochs = 20\n",
    "        # epochs = 1  # XXX: test!\n",
    "        dataset = dataset.shuffle(10000, seed=SEED).repeat(epochs).batch(BATCH_SIZE)\n",
    "        fantasy_state = initialize_fantasy_state(bm, BATCH_SIZE, SEED)\n",
    "        optimizer = tf.optimizers.Adam()\n",
    "        fantasy_state = train(bm, optimizer, dataset, fantasy_state)\n",
    "    else:\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                bm, fantasy_state = pickle.load(f)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f'[WARNING]: Cannot find file \"{cache_path}\", create new file on that path.')\n",
    "            bm, fantasy_state = build_and_train(epochs, cache_path=None)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump((bm, fantasy_state), f)\n",
    "    return bm, fantasy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bm, base_fantasy_state = build_and_train(1, cache_path='../dat/base_bm_for_rg_flow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reconstruction_error(base_bm, X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "history = History()\n",
    "bm = deepcopy(base_bm)\n",
    "fantasy_state = deepcopy(base_fantasy_state)\n",
    "iter_step = 0\n",
    "\n",
    "def log(iter_step):\n",
    "    history.log(iter_step, 'ambient_latent_kernel', bm.ambient_latent_kernel.numpy())\n",
    "    history.log(iter_step, 'latent_latent_kernel', bm.latent_latent_kernel.numpy())\n",
    "    history.log(iter_step, 'ambient_bias', bm.ambient_bias.numpy())\n",
    "    history.log(iter_step, 'latent_bias', bm.latent_bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite loop of incrementing\n",
    "while bm.latent_size <= 512:\n",
    "    print(f'The {iter_step + 1}th interation......')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    epochs = 10  # enough epochs for ensuring the convergence of training.\n",
    "    # epochs = 1  # XXX: test!\n",
    "    dataset = dataset.shuffle(10000, seed=SEED).repeat(epochs).batch(BATCH_SIZE)\n",
    "    inc_bm, inc_fantasy_state = enlarge_latent(bm, fantasy_state, INCREMENT)\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "    inc_fantasy_state = train(inc_bm, optimizer, dataset, inc_fantasy_state)\n",
    "\n",
    "    bm, fantasy_state, iter_step = inc_bm, inc_fantasy_state, iter_step + 1\n",
    "\n",
    "    log(iter_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current latent size:', bm.latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sorted(list(history.logs.keys()))\n",
    "kernel_diff_hist = []\n",
    "for i, j in zip(steps[:-1], steps[1:]):\n",
    "    U_i = history.logs[i]['ambient_latent_kernel'][:, :LATENT_SIZE]\n",
    "    U_j = history.logs[j]['ambient_latent_kernel'][:, :LATENT_SIZE]\n",
    "    kernel_diff_hist.append(U_j - U_i)\n",
    "kernel_diff_hist = np.stack(kernel_diff_hist, axis=0)\n",
    "kernel_diff_hist = ExponentialMovingAverage(0.9)(kernel_diff_hist, axis=0).numpy()\n",
    "\n",
    "plt.plot(steps[1:], np.zeros_like(steps[1:]), '--', label='zero')\n",
    "\n",
    "def plot_confidence_region(confidence, **plot_kwargs):\n",
    "    lower = [np.quantile(x.reshape([-1]), (1 - confidence) / 2) for x in kernel_diff_hist]\n",
    "    upper = [np.quantile(x.reshape([-1]), 1 - (1 - confidence) / 2) for x in kernel_diff_hist]\n",
    "    plt.fill_between(steps[1:], lower, upper,\n",
    "                     label=f'{(confidence * 100):.2f}% confidence region',\n",
    "                     **plot_kwargs)\n",
    "\n",
    "plot_confidence_region(0.6827, alpha=0.5)\n",
    "plot_confidence_region(0.9544, alpha=0.25)\n",
    "plot_confidence_region(0.9973, alpha=0.25)\n",
    "\n",
    "plt.title('Averaged kernel difference history')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}